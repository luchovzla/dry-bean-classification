<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Neural Network</title>

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" 
  integrity="sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l" 
  crossorigin="anonymous">

  <!-- Our CSS -->
  <link rel="stylesheet" href="../css/style.css">

</head>

<body>

  <div class="container">
    <nav style="background-color:rgb(201, 168, 231) !important" class="p-1 navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
        <img src="../../Images/comp_1_2.gif" height="42" width="42">
        <a class="navbar-header" href="#"></a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown"
          aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNavDropdown">
          <ul class="navbar-nav">
            <li class="nav-item">
              <a class="nav-link font-weight-bold" href="/index.html" style="color: ghostwhite;">Dry Bean Categorization<span class="sr-only">(current)</span></a>
            </li>
            <li class="nav-item dropdown">
              <a class="nav-link active dropdown-toggle" href="#" id="navbarDropdownMenuLink" role="button"
                data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">ML Models</a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
                <a class="dropdown-item" target="_blank" href="random_forest.html">Random Forest</a>
                <a class="dropdown-item" target="_blank" href="mlr.html">Multiple Linear Regression</a>
                <a class="dropdown-item" target="_blank" href="neural_network.html">Neural Network</a>
                <a class="dropdown-item" target="_blank" href="knn.html">K Nearest Neighbors (KNN)</a>
              </div>
            </li>
            <li class="nav-item">
              <a class="nav-link" target="_blank" href="analysis.html" style="color: ghostwhite;">Analysis</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" target="_blank" href="credits.html" style="color: ghostwhite;">Credits</a>
            </li>
          </ul>
        </div>
      </nav>
    
    <div style="background-color:rgb(169, 212, 175) !important" class= jumbotron text-center">

      <div>
        <h1 style="color: rgb(42, 116, 42)">Classifying different types of dry beans using a Neural Network with different optimizers</h1>
        <img src='../../Images/1200px-Neural_network_example.png' class='col-lg-12 img-center'></img>

        <br>
        <h6 align='center'>Figure 1: Representation of a simple neural network. Credit: Wikipedia</h6>
      </div>
    </div>

    <div>
        <p>One of the Machine Learning algorithms used to tackle this problem was a Neural Network. A multivariate classification problem such as the one presented in this dataset is ideal for this algorithm to solve.</p>
        <p>The determination of the architecture of a neural network is highly experimentative. The number of nodes and layers to use is a parameter that can be highly tuned and a balance between computer usage/model size and accuracy must be achieved.</p>
        <p>For this model, a shallow neural network (a neural network with a single hidden layer) was used. Neural network architecture theory says that the number of nodes in each layer should be between the number of input dimensions and output dimensions, decreasing as the depth of the network increases. Therefore, a hidden layer with 14 nodes was chosen as the architecture of this neural network.</p>
        <h2>Effect of using different optimizers in a Neural Network</h2>
        <br>
        <p>One of the parameters in a neural network model that can be also modified when compiling the model is the optimizer used. Optimizers are methods or algorithms used to change the attributes of a neural network, such as weights and learning rate, to reduce losses.</p>
        <p>Four different optimizers: Adam, Nadam, RMSProp and SGD were studied, using the same network and ran over 50 epochs, with shuffling enabled.</p> 
        <br>
        <table border="1" class="dataframe" align="center">
          <thead>
            <tr style="text-align: center;">
              <th>Optimizer</th>
              <th>Accuracy</th>
              <th>Loss</th>
            </tr>
          </thead> 
          <tbody> 
            <tr>
              <th>Adam</th>
              <td>0.919483</td>
              <td>0.233923</td>
            </tr>
            <tr>
              <th>Nadam</th>
              <td>0.916838</td>
              <td>0.235692</td> 
            </tr>
            <tr>
              <th>RMSProp</th>
              <td>0.909198</td>
              <td>0.257166</td>
            </tr>
            <tr>
              <th>SGD</th>
              <td>0.889509</td>
              <td>0.358123</td>
            </tr>
          </tbody>
        </table>
        <br>
        <h6 align='center'>Table 1: Model accuracy and losses for 4 different model optimizers.</h6>
        <br>
        <p>It can be appreciated that the Adam model has the lowest loss and highest accuracy of all models, though 3 of them (Adam, Nadam and RMSProp) performed roughly equal. The SGD optimizer had an inferior performance, both in accuracy and losses.</p>
        <p>The performance of each optimizer was plotted over the number of epochs, measured by each model's accuracy and losses.</p>
        <img class="img-center" src="img/optimizer_accuracy.png">
        <h6 align='center'>Figure 2: Model accuracy vs. epochs ran using 4 different model optimizers.</h6>
        <br>
        <p>After 50 iterations of model training (also known as epochs), it can be appreciated that the Adam, Nadam and RMSProp optimizers have a virtually identical performance over time. SGD, on the other hand, noticeably trails behind the former three.</p>
        <img class="img-center" src="img/optimizer_loss.png">
        <h6 align='center'>Figure 3: Model loss vs. epochs ran using 4 different model optimizers.</h6>
        <br>
        <p>When the losses are plotted vs. epochs ran, the behavior of the SGD-optimized model reaffirms that, for this problem, it is the inferior optimizer, presenting a consistently higher loss than the other 3 optimizers.</p>
        <p>In conclusion, the best studied architecture for this neural network would be a single hidden layer with 14 nodes, using the Adam optimizer.</p>
    </div>
  </div>  
  
  <!-- Bootstrap JS -->
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" 
  integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" 
  crossorigin="anonymous"></script>

  <!-- Bootstrap JS -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.bundle.min.js" 
  integrity="sha384-Piv4xVNRyMGpqkS2by6br4gNJ7DXjqk09RmUpJ8jgGtD7zP9yug3goQfGII0yAns" 
  crossorigin="anonymous"></script>

</body>

</html>